{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    " \n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from functools import wraps\n",
    "\n",
    "\n",
    "def fn_timer(function):\n",
    "    @wraps(function)\n",
    "    def function_timer(*args, **kwargs):\n",
    "        t0 = datetime.datetime.now().microsecond\n",
    "        print(datetime.datetime.now())\n",
    "        result = function(*args, **kwargs)\n",
    "        t1 = datetime.datetime.now().microsecond\n",
    "        print(\"Total time running %s: %s seconds\" %\n",
    "              (function.__name__, str(t1 - t0))\n",
    "              )\n",
    "        return result\n",
    "\n",
    "    return function_timer\n",
    "\n",
    "\n",
    "URL = 'http://sh.lianjia.com'\n",
    "xiaoqu_url = 'http://sh.lianjia.com/xiaoqu/d'\n",
    "detail_url = 'http://sh.lianjia.com/xiaoqu/5011102207057.html'\n",
    "\n",
    "\n",
    "\n",
    "def download_page(url, retries=3):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0'}\n",
    "    try:\n",
    "        data = requests.get(url, headers=headers).content\n",
    "    except Exception as err:\n",
    "        print(\"fail to dowload the page, reason: \")\n",
    "        print(err)\n",
    "        if retries>0:\n",
    "            print(\"try again\")\n",
    "            return download_page(url,retries-1)\n",
    "        else:\n",
    "            print(\"failed in scaping : %s\" % url)\n",
    "            return ''        \n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def parse_page(html):\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    link_list = []\n",
    "    name_list = []\n",
    "    xiaoqu_list = soup.findAll('div', attrs={'class': 'info-panel'})\n",
    "    # print(xiaoqu_list)\n",
    "\n",
    "    for xiaoqu in xiaoqu_list:\n",
    "        link = xiaoqu.find('a').get('href')\n",
    "        name = xiaoqu.find('a').get('title')\n",
    "        link_list.append(link)\n",
    "    \n",
    "    for link in link_list:\n",
    "        \n",
    "        parse_detail_page(link)\n",
    "\n",
    "\n",
    "def parse_detail_page(link):\n",
    "    result = ' '\n",
    "    detail_link = URL + link\n",
    "    detail_soup = BeautifulSoup(download_page(detail_link),\"html.parser\")\n",
    "    # get the price\n",
    "    price = detail_soup.find('span',attrs={'class':'p'}).getText().strip()\n",
    "\n",
    "    # get name, latitude and longitude\n",
    "    coord = detail_soup.find('a',attrs={'class':'actshowMap'}).get('xiaoqu')\n",
    "\n",
    "    coord = coord.split(',')\n",
    "\n",
    "    latitude = coord[1]\n",
    "    longitude = coord[0].strip('[')\n",
    "    # name = coord[2].strip(']').strip(\"'\")\n",
    "    name = coord[2][2:-2]\n",
    "\n",
    "    # get address\n",
    "    adr = detail_soup.find('span',attrs={'class':'adr'}).getText().strip()\n",
    "\n",
    "    result = name + ' ' + link + ' ' + price + ' ' + latitude + ' ' + longitude + ' ' + adr\n",
    "\n",
    "    # get other info\n",
    "    others = []\n",
    "    other_info = detail_soup.findAll('span',attrs={'class':'other'})\n",
    "    # print(other_info)\n",
    "    for info in other_info:\n",
    "        # print(info.getText())\n",
    "        # print(info.getText().strip())\n",
    "        others.append(info.getText().strip())\n",
    "    \n",
    "    \n",
    "    i = 1\n",
    "    for other in others:\n",
    "        if i<3:\n",
    "            result += ' ' + other\n",
    "            i += 1\n",
    "        else:\n",
    "            break\n",
    "  \n",
    "    result +=  '\\n'\n",
    "    print(result)\n",
    "    with open('shanghai_old.txt', 'a') as f:\n",
    "        f.write(result)\n",
    "    \n",
    "    # time.sleep(0.5)\n",
    "\n",
    "def parse_detail_page_test(html):\n",
    "\n",
    "    detail_soup = BeautifulSoup(html,\"html.parser\")\n",
    "    # get the price\n",
    "    price = detail_soup.find('span',attrs={'class':'p'}).getText().strip()\n",
    "\n",
    "    # get name, latitude and longitude\n",
    "    coord = detail_soup.find('a',attrs={'class':'actshowMap'}).get('xiaoqu')\n",
    "\n",
    "    coord = coord.split(',')\n",
    "\n",
    "    latitude = coord[1]\n",
    "    longitude = coord[0].strip('[')\n",
    "    # name = coord[2].strip(']').strip(\"'\")\n",
    "    name = coord[2][2:-2]\n",
    "\n",
    "    # get address\n",
    "    adr = detail_soup.find('span',attrs={'class':'adr'}).getText().strip()\n",
    "\n",
    "#     result = name + ' ' + link + ' ' + price + ' ' + latitude + ' ' + longitude + ' ' + adr\n",
    "    result = name + ' ' + price + ' ' + latitude + ' ' + longitude + ' ' + adr\n",
    "\n",
    "\n",
    "    # get other info\n",
    "    others = []\n",
    "    other_info = detail_soup.findAll('span',attrs={'class':'other'})\n",
    "    # print(other_info)\n",
    "    for info in other_info:\n",
    "        # print(info.getText())\n",
    "        # print(info.getText().strip())\n",
    "        others.append(info.getText().strip())\n",
    "    \n",
    "    \n",
    "    i = 1\n",
    "    for other in others:\n",
    "        if i<3:\n",
    "            result += ' ' + other\n",
    "            i += 1\n",
    "        else:\n",
    "            break\n",
    "  \n",
    "    result +=  '\\n'\n",
    "    print(result)\n",
    "    with open('shanghai_new.txt', 'a') as f:\n",
    "        f.write(result)\n",
    "    \n",
    "    # time.sleep(0.5)\n",
    "    \n",
    "    \n",
    "def parse_page_test(html):\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    link_list = []\n",
    "    name_list = []\n",
    "    xiaoqu_list = soup.findAll('div', attrs={'class': 'info-panel'})\n",
    "    # print(xiaoqu_list)\n",
    "\n",
    "    for xiaoqu in xiaoqu_list:\n",
    "        link = xiaoqu.find('a').get('href')\n",
    "        detail_link = URL + link\n",
    "        link_list.append(detail_link)\n",
    "    \n",
    "    return link_list\n",
    "\n",
    "@fn_timer\n",
    "def m1():\n",
    "    \n",
    "    url = xiaoqu_url + '1'\n",
    "    html = download_page(url)\n",
    "    urls = parse_page_test(html)\n",
    "     \n",
    "    pool = ThreadPool()\n",
    "\n",
    "#     results = pool.map(urllib.request.urlopen,urls)\n",
    "    results = pool.map(download_page,urls)\n",
    "#     print(results)\n",
    "    for http_obj in results:\n",
    "        parse_detail_page_test(http_obj)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    print('main1 ended')\n",
    "    # detail_soup = BeautifulSoup(download_page(detail_url),\"html.parser\")\n",
    "    # coord = detail_soup.find('a',attrs={'class':'actshowMap'}).get('xiaoqu')\n",
    "    \n",
    "    # parse_detail_page('hh', detail_url)\n",
    "    # a = \"'绿地威廉公寓']\"\n",
    "    # print(a)\n",
    "    # print(a[1:-2])\n",
    "    # print(a.strip(\"]\").strip(\"'\"))\n",
    "    # a = a.strip(\"'\")\n",
    "    # print(a)\n",
    "\n",
    "#     url = xiaoqu_url + '1'\n",
    "#     html = download_page(url)\n",
    "#     parse_page(html)\n",
    "\n",
    " \n",
    "#     for i in range(89,101):\n",
    "        \n",
    "#         url = xiaoqu_url + str(i)\n",
    "#         print(\"start scraping \" + url)\n",
    "#     # url = xiaoqu_url\n",
    "#         html = download_page(url)\n",
    "#         parse_page(html)\n",
    "#         print(\"# ----------------- got page %d !!!\" % i)\n",
    "\n",
    "    # while url:\n",
    "    #     html = download_page(url)\n",
    "    #     movies, url = parse_page(html)\n",
    "    #     print(movies)\n",
    "        #print('--------------------------------------------------------------------------------------')\n",
    "@fn_timer\n",
    "def m2():\n",
    "    \n",
    "#     url = xiaoqu_url + '1'\n",
    "#     html = download_page(url)\n",
    "#     urls = parse_page_test(html)\n",
    "     \n",
    "#     pool = ThreadPool()\n",
    "\n",
    "# #     results = pool.map(urllib.request.urlopen,urls)\n",
    "#     results = pool.map(download_page,urls)\n",
    "# #     print(results)\n",
    "#     for http_obj in results:\n",
    "#         parse_detail_page_test(http_obj)\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "\n",
    "#     print('main ended')\n",
    "    # detail_soup = BeautifulSoup(download_page(detail_url),\"html.parser\")\n",
    "    # coord = detail_soup.find('a',attrs={'class':'actshowMap'}).get('xiaoqu')\n",
    "    \n",
    "    # parse_detail_page('hh', detail_url)\n",
    "    # a = \"'绿地威廉公寓']\"\n",
    "    # print(a)\n",
    "    # print(a[1:-2])\n",
    "    # print(a.strip(\"]\").strip(\"'\"))\n",
    "    # a = a.strip(\"'\")\n",
    "    # print(a)\n",
    "\n",
    "    url = xiaoqu_url + '1'\n",
    "    html = download_page(url)\n",
    "    parse_page(html)\n",
    "    print('main2 ended')\n",
    "\n",
    "#     for i in range(89,101):       \n",
    "#         url = xiaoqu_url + str(i)\n",
    "#         print(\"start scraping \" + url)\n",
    "#     # url = xiaoqu_url\n",
    "#         html = download_page(url)\n",
    "#         parse_page(html)\n",
    "#         print(\"# ----------------- got page %d !!!\" % i)\n",
    "\n",
    "    # while url:\n",
    "    #     html = download_page(url)\n",
    "    #     movies, url = parse_page(html)\n",
    "    #     print(movies)\n",
    "        #print('--------------------------------------------------------------------------------------')\n",
    "def main():\n",
    "    m1()\n",
    "    m2()\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    print('good')\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
